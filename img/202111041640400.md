# 多模态图像+声音研读

音频顶会：icassp，interspeech, icme

音频期刊：IEEE/ACM Trans. Audio, Speech, Language Processing, IEEE Trans. Signal Processing, Elsevier Signal Processing, Speech Communication等

视觉顶会：cvpr, iccv, aaai, nips，iclr

1.微软亚洲研究院自然语言计算组发布的一项创新的研究成果——开放领域视频生成预训练模型，基于 3D 注意力  机制，克服了视频帧连续性建模的挑战，可秒级实现基于文字脚本生成视频。

技术原理：基于VQ-VAE（相对于GAN来说更接近nlp的研究思路）

视频和图片在这里并没有本质区别，因为视频可以被切分成很多个视频帧，即图片。利用 VQ-VAE 算法模型可以将每一个视频帧编码成离散向量表示，这样图片信息就可以对应到相应的文本，从而序列化为 NLP 最擅长处理的 token，充分利用现有的 NLP 模型和算法。在大规模数据预训练之后，再基于 VQ-VAE 模型将离散序列反向还原成视频帧，并将所有帧连在一起，就形成了可视化的视频。

总结：主要是根据输入的语言文字来输出与语言文字相关的视频，通过已有视频合成或者说无中生有。

论文链接：[[2104.08860\] CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval (arxiv.org)](https://arxiv.org/abs/2104.08860)

2.

## Deep multimodal learning for Audio-Visual Speech Recognition (IEEE)

[IEEE Xplore Full-Text PDF:](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178347)

用于视听语音识别的深度多模态学习

### 摘要

新模型：Audio-Visual Automatic Speech Recognition (AV-ASR)视听自动语言识别

方法：①先用单模态的深度神经网络训练并将他们在最后的隐藏层融合得到一个joint feature space(有另外一个深层网络)

②一种新的深度网络架构（用a bilinear softmax layer来说明各模式之间的class specific correlations类特定相关性）

将以上两个方法的数据融合，PER 34.03%

突破点：与之前的AV-ASR的研究主要是在嘈杂的环境中，视觉信息至关重要，这篇文章主要阐明了在干净的语音场景中，视觉也是有帮助的



### 数据集和特征提取

数据集要求：音频录音和人说话的视频

验证数据集：IBM AV-ASR Large Vocabulary Studio Dataset （人为分为训练集和测试集2h）

特征提取：360维音频特征向量和540维的视频特征向量.

​                  音频：以每秒100帧的速度提取24个MFCC系数，将9个MFCC系数的连续框架堆叠并投影到40个维度

​                             LDA矩阵。音频神经网络的输入是通过将±4个LDA帧连接到感兴趣的中心帧the central frame of    

​                             interest,从而产生一个360维的音频特征向量。

​                  视频：用openCV的Viola-Jones算法检测人脸，再用openCV中的口腔检测模型进行口腔雕刻，利用LDA     

​                             矩阵降维道60，在the central frame of interest中扩展上下四帧（匹配音频信号），得到540帧        

​                             的视频特征向量。

每个音频+视频帧都被标注为1328个代表上下文相关音素的目标中的一个。在这1328类的测量分类错误率，称为PER。



### 单模态DNNS与特征融合

受监督的多模态场景中，有N个标注例子的训练集S和C类

![image-20211101095109654](images/image-20211101095109654.png)

两个x向量对应第一模态和第二模态的特征向量

最大交叉熵：

![image-20211101095508229](images/image-20211101095508229.png)

标框处为给定分类y对于两个x特征向量的后验概率,ti是分类的目标

实际方法：第一种多模态方法是将视频和音频特征分开研究，有两个单独的神经网络，网络在交叉熵的目标下用随机的梯度下降法进行优化，将这两个网络的最终隐藏层的输出连接起来，形成了一个联合的视听特征表示。如下图：

![image-20211101100141322](images/image-20211101100141322.png)

当一个深或浅网络(仅限softmax)在融合空间中训练到目标时，这个特征空间将保持固定。为了保持特征空间维度可管理，我们将单独的音频和视频网络配置为具有低维度的最终隐藏层的网络。

在融合特征空间中构建深度神经网络可以达到35.77%，但是仅在特征空间中构建softmax层可以达到35.83%，深聚变和浅聚变在PER上结果大致相同。

结果：

![image-20211101100618773](images/image-20211101100618773.png)

### BILINEAR DEEP NEURAL NETWORK

双线性深度神经网络（之前是单独训练，这里主要是联合训练并介绍bilinear bimodal双线性双峰DNN）

对上一节最后的实现方法进行阐述

对于DNN用σ表示非线性函数（本文中为sigmoid),vl是输入单元，hl是输出单元（第l层），中间层的独立网络如下（即在融合之前就是普通的深度神经网络的输入输出）：

![image-20211101101912512](images/image-20211101101912512.png)

融合发生在最后一个隐藏层，在那里后端通过双线性项捕获由DNN层产生的两种模式的中间非线性特征之间的相关性。预定义![image-20211101102242193](images/image-20211101102242193.png)，融合过程如下：

![image-20211101102400695](images/image-20211101102400695.png)

#### Factored Bilinear Softmax  因子双线性softmax

目的：降低由于类数增加带来的双线性模型的计算复杂度，提出双线性项的因式分解。我们的动机是Canonical Correlation Analysis(CCA),典型相关分析：

![image-20211101103105193](images/image-20211101103105193.png)

其中![image-20211101103316848](images/image-20211101103316848.png)diag(wy)是一个wy在其对角的对角矩阵，F表示融合向量空间的维数。

![image-20211103112314045](images/image-20211103112314045.png)其中λ是正则化系数

对于固定的（U1，U2），在F维融合空间中实行线性超平面。决策函数是基于两个模态的单独贡献和融合空间![image-20211103105514894](images/image-20211103105514894.png)的联合表示来决定的。

#### Factored Bilinear Softmax with sharing

  当我们想要预测的类被组织为深度为2的树结构的叶子时，我们可以通过在具有相同父节点的叶子之间共享权值来进一步降低计算复杂度。

  在AV-ASR的情况中作为1328个语境音素状态被看为叶子节点，其中父节点对应42个不同的音位范畴。在这种情况下，我们共享双线性的叶子有相同的父节点。通过在AV-ASR中这样做，我们只考虑了音素水平上的音频和视觉通道之间的相关性，而没有考虑上下文状态的细粒度网格。我们可以把这种共享看作是音素级的池操作。

更正式地说，假设标签集

将Y划分为G个不重叠的群![image-20211101104925312](images/image-20211101104925312.png)，我们假设:![image-20211101104934110](images/image-20211101104934110.png)因此，我们将联合表示需要学习的权值从C × F减少到G × F。



### 带因子的反向传播   具有共享的双线性DNN

反向传播算法和更新规则：

本篇文章的分类是有y个叶子节点的树结构（叶子y和父节点g），用符号g(y)来表示y所属的组,然后设置Rootg(y)=1,Rootg = 0, g = 1...G,g≠g(y)。

具有共享的双线性softmax，在标签级和组级跟踪错误：

![image-20211101105609720](images/image-20211101105609720.png)

![image-20211101105630591](images/image-20211101105630591.png)

对于双线性softmax之前的层，有视频流和音频流的双重投影，需要计算：

![image-20211101105809940](images/image-20211101105809940.png)

2->1是双线性在网络之间传递的信息，通过这种方式，一个网络来影响另外一个网络的权重。（即在反向传播的过程中进行相应的误差传回）

### 结论数据

双线性网络的结构： [archa|archv|F]

archa：音频网络 

archv：视频网络 

F：融合空间维度

Arch = [360, 500, 500, 200, 1328|540, 500, 500, 200, 1328 |F = 200]

Arch1 = [360, 600, 600, 400, 100, 1328|540, 600, 600, 400, 100, 1328|F = 100]

Arch2 = [360, 500, 500, 500, 500, 500, 200, 1328|540, 500, 500, 500, 500, 500, 200, 1328|F = 200]

如图所示，每个体系结构单独对双峰DNN（即Arch,Arch1,Arch2的PER)没有改善，但对三个体系结构的后端进行平均，我们获得了一个很小的增益。通过对双峰网络和双线性双峰网络的后端进行平均，获得了1.8%的绝对增益，表明双线性网络与双峰网络具有不相关的误差。

![image-20211101110705344](images/image-20211101110705344.png)





别人的论文研读网址（可借鉴参考）:

[GitHub - xmu-xiaoma666/FightingCV-Paper-Reading: ⭐⭐⭐FightingCV Paper Reading, which helps you understand the most advanced research work in an easier way 🍀 🍀 🍀](https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading)







github的token:

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
